---
title: "Publications and Patents"
date: 2024-08-25
draft: false
menu:
  main:
    name: "Publications"
    weight: 5
---

{{< rawhtml >}}

<style>
    ul li { margin-bottom: 15px; }
</style>

<p><h1>Publications</h1></p>

<p><sup>*</sup> indicates equal contribution, <sup>#</sup> indicates corresponding author</p>

<p><h2>2024</h2></p>

<ul>

<li>
<i>
Enabling Parallelism Hot Switching for Efficient Training of Large Language Models
</i>
<br>
Hao Ge<sup>*</sup>, <b>Fangcheng Fu</b><sup>*#</sup>, Haoyang Li, Xuanyu Wang, Sheng Lin, Yujie Wang, Xiaonan Nie, Hailin Zhang, Xupeng Miao, Bin Cui<sup>#</sup>
<br>
<b>SOSP 2024</b>
</li>

<li>
<i>
ProjPert: Projection-based Perturbation for Label Protection in Split Learning based Vertical Federated Learning
</i>
<br>
<b>Fangcheng Fu</b>, Xuanyu Wang, Jiawei Jiang, Huanran Xue, and Bin Cui
<br>
<b>TKDE 2024</b>
</li>

<li>
<i>
Improving Automatic Parallel Training via Balanced Memory Workload Optimization
</i>
<br>
Yujie Wang, Youhe Jiang, Xupeng Miao, <b>Fangcheng Fu</b><sup>#</sup>, Shenhan Zhu, Xiaonan Nie, Yaofeng Tu, Bin Cui<sup>#</sup>
<br>
<b>TKDE 2024</b>
</li>

<li>
<i>
Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion Inference
</i>
<br>
Zihao Yu, Haoyang Li, <b>Fangcheng Fu</b>, Xupeng Miao, Bin Cui
<br>
<b>AAAI 2024</b>
</li>

<li>
<i>
Reviving Efficient Attention for Long Context Language Modeling: A Survey
</i>
<br>
Xupeng Miao, Shenhan Zhu, <b>Fangcheng Fu</b>, Ziyu Guo, Zhi Yang, Yaofeng Tu, Zhihao Jia, Bin Cui
<br>
<b>IJCAI 2024</b>
</li>

<li>
<i>
Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning
</i>
<br>
Yuxiang Wang, Xiao Yan, Chuang Hu, Quanqing Xu, Chuanhui Yang, <b>Fangcheng Fu</b>, Wentao Zhang, Hao Wang, Bo Du, Jiawei Jiang
<br>
<b>ICDE 2024</b>
</li>

</ul>


<p><h2>2023</h2></p>

<ul>

<li>
<i>
Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent
</i>
<br>
Xiaonan Nie, Yi Liu, <b>Fangcheng Fu</b><sup>#</sup>, Jinbao Xue, Dian Jiao, Xupeng Miao, Yangyu Tao, Bin Cui<sup>#</sup>
<br>
<b>VLDB 2023</b>
</li>

<li>
<i>
OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning
</i>
<br>
Youhe Jiang, <b>Fangcheng Fu</b><sup>#</sup>, Xupeng Miao, Xiaonan Nie, Bin Cui<sup>#</sup>
<br>
<b>IJCAI 2023</b>
</li>

<li>
<i>
KVSAgg: Secure Aggregation of Distributed Key-Value Sets
</i>
<br>
Yuhan Wu, Siyuan Dong, Yi Zhou, Yikai Zhao, <b>Fangcheng Fu</b>, Tong Yang, Chaoyue Niu, Fan Wu, Bin Cui
<br>
<b>ICDE 2023</b>
</li>

<li>
<i>
P2CG: A Privacy Preserving Collaborative Graph Neural Network Training Framework
</i>
<br>
Xupeng Miao, Wentao Zhang, Yuezihan Jiang, <b>Fangcheng Fu</b>, Yingxia Shao, Lei Chen, Yangyu Tao, Gang Cao, Bin Cui
<br>
<b>VLDB Journal 2023</b>
</li>

<li>
<i>
Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion Inference
</i>
<br>
Zihao Yu, Haoyang Li, <b>Fangcheng Fu</b>, Xupeng Miao, Bin Cui
<br>
<b>MLSys Workshop NeurIPS 2023</b>
</li>

</ul>

<p><h2>2022</h2></p>

<ul>

<li>
<i>
Towards Communication-efficient Vertical Federated Learning Training via Cache-enabled Local Update
</i>
<br>
<b>Fangcheng Fu</b>, Xupeng Miao, Jiawei Jiang, Huanran Xue, Bin Cui
<br>
<b>VLDB 2022</b>
</li>

<li>
<i>
BlindFL: Vertical Federated Machine Learning without Peeking into Your Data
</i>
<br>
<b>Fangcheng Fu</b>, Huanran Xue, Yong Cheng, Yangyu Tao, Bin Cui
<br>
<b>SIGMOD 2022</b>
</li>

<li>
<i>
VF-PS: How to Select Important Participants in Vertical Federated Learning, Efficiently and Securely?
</i>
<br>
Jiawei Jiang, Lukas Burkhalter, <b>Fangcheng Fu</b><sup>#</sup>, Bo Li, Bolin Ding, Bo Du, Anwar Hithnawi, Ce Zhang
<br>
<b>NeurIPS 2022</b>
</li>

<li>
<i>
Analyzing Online Transaction Networks with Network Motifs
</i>
<br>
Jiawei Jiang, Yusong Hu, Xiaosen Li, Wen Ouyang, Zhitao Wang, <b>Fangcheng Fu</b><sup>#</sup>, Bin Cui
<br>
<b>SIGKDD 2022</b>
</li>

<li>
<i>
K-Core Decomposition on Super Large Graphs with Limited Resources
</i>
<br>
Shicheng Gao, Jie Xu, Xiaosen Li, <b>Fangcheng Fu</b>, Wentao Zhang, Wen Ouyang, Yangyu Tao, Bin Cui
<br>
<b>ACM SAC 2022</b>
</li>

</ul>

<p><h2>2021</h2></p>

<ul>

<li>
<i>
VF<sup>2</sup>Boost: Very Fast Vertical Federated Gradient Boosting for Cross-Enterprise Learning
</i>
<br>
<b>Fangcheng Fu</b>, Yingxia Shao, Lele Yu, Jiawei Jiang, Huanran Xue, Yangyu Tao, Bin Cui
<br>
<b>SIGMOD 2021</b>
</li>

</ul>

<p><h2>2020</h2></p>

<ul>

<li>
<i>
Don’t Waste Your Bits! Squeeze Activations and Gradients for Deep Neural Networks via TinyScript
</i>
<br>
<b>Fangcheng Fu</b>, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, Bin Cui
<br>
<b>ICML 2020</b>
</li>

<li>
<i>
SKCompress: Compressing Sparse and Nonuniform Gradient in Distributed Machine Learning
</i>
<br>
Jiawei Jiang<sup>*</sup>, <b>Fangcheng Fu</b><sup>*</sup>, Tong Yang, Yingxia Shao, Bin Cui
<br>
<b>VLDB Journal 2020</b>
</li>

</ul>

<p><h2>2019</h2></p>

<ul>

<li>
<i>
An Experimental Evaluation of Large Scale GBDT Systems
</i>
<br>
<b>Fangcheng Fu</b>, Jiawei Jiang, Yingxia Shao, Bin Cui
<br>
<b>VLDB 2019</b>
</li>

<li>
<i>
Distributed Gradient Boosting Decision Tree Algorithm for High-dimensional and Multi-classification Problems (in Chinese)
</i>
<br>
Jiawei Jiang, <b>Fangcheng Fu</b>, Yingxia Shao, Bin Cui
<br>
<b>Journal of Software (软件学报) 2019</b>
</li>

</ul>

<p><h2>2018</h2></p>

<ul>

<li>
<i>
SketchML: Accelerating Distributed Machine Learning with Data Sketches
</i>
<br>
Jiawei Jiang, <b>Fangcheng Fu</b>, Tong Yang, Bin Cui
<br>
<b>SIGMOD 2018</b>
</li>

<li>
<i>
DimBoost: Boosting Gradient Boosting Tree to Higher Dimensions
</i>
<br>
Jiawei Jiang, Bin Cui, Ce Zhang, <b>Fangcheng Fu</b>
<br>
<b>SIGMOD 2018</b>
</li>

</ul>

<p><h1>Patents</h1></p>

<ul>
<li>
基于深度神经网络最小方差梯度量化压缩及图像处理方法. ZL 2019 1 1029711.0
</il>
<li>
一种数据处理方法、装置、设备及计算机可读存储介质. ZL 2021 1 0576191.6
</il>
<li>
基于联邦学习的数据传输方法、装置以及可读存储介质. ZL 2021 1 0680161.X
</il>
<li>
基于联邦神经网络模型的数据处理方法、相关设备及介质. ZL 2021 1 0531392.4
</il>
<li>
联邦模型训练方法、装置、终端设备以及存储介质. ZL 2022 1 0363190.8
</il>
<li>
多方安全计算方法、装置、设备及存储介质. ZL 2021 1 0503941.7
</il>
<li>
联邦神经网络模型的训练方法、装置、设备及存储介质. ZL 2020 1 1167325.0
</il>
<li>
数据集合处理方法、数据处理方法、装置及存储介质. ZL 2021 1 0541183.8
</il>
</ul>

{{< /rawhtml >}}
