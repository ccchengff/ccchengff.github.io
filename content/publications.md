---
title: "Publications and Patents"
draft: false
menu:
  main:
    name: "Publications"
    weight: 5
---

{{< rawhtml >}}

<style>
    ul li { margin-bottom: 15px; }
</style>

<p><h1>Publications</h1></p>

<p><sup>*</sup> indicates equal contribution, <sup>#</sup> indicates corresponding author</p>

<p><h2>2026</h2></p>

<ul>

<li>
<i>
Hydraulis: Balancing Large Transformer Model Training via Co-designing Parallel Strategies and Data Assignment
</i>
<br>
Haoyang Li, <b>Fangcheng Fu</b><sup>#</sup>, Sheng Lin, Hao Ge, Xuanyu Wang, Jiawen Niu, Jinbao Xue, Yangyu Tao, Di Wang, Jie Jiang, Bin Cui<sup>#</sup>
<br>
<b>SIGMOD 2026</b>
</li>

</ul>

<p><h2>2025</h2></p>

<ul>

<li>
<i>
Malleus: Straggler-Resilient Hybrid Parallel Training of Large-scale Models via Malleable Data and Model Parallelization
</i>
<br>
Haoyang Li<sup>*</sup>, <b>Fangcheng Fu</b><sup>*#</sup>, Hao Ge, Sheng Lin, Xuanyu Wang, Jiawen Niu, Yujie Wang, Hailin Zhang, Xiaonan Nie, Bin Cui<sup>#</sup>
<br>
<b>SIGMOD 2025</b>
</li>

<li>
<i>
PQCache: Product Quantization-based KVCache for Long Context LLM Inference
</i>
<br>
Hailin Zhang, Xiaodong Ji, Yilin Chen, <b>Fangcheng Fu</b>, Xupeng Miao, Xiaonan Nie, Weipeng Chen, Bin Cui
<br>
<b>SIGMOD 2025</b>
</li>

<li>
<i>
Memo: Fine-grained Tensor Management For Ultra-long Context LLM Training
</i>
<br>
Pinxue Zhao, Hailin Zhang, <b>Fangcheng Fu</b><sup>#</sup>, Xiaonan Nie, Qibin Liu, Fang Yang, Yuanbo Peng, Dian Jiao, Shuaipeng Li, Jinbao Xue, Yangyu Tao, Bin Cui<sup>#</sup>
<br>
<b>SIGMOD 2025</b>
</li>

<li>
<i>
LobRA: Multi-tenant Fine-tuning over Heterogeneous Data
</i>
<br>
Sheng Lin<sup>*</sup>, <b>Fangcheng Fu</b><sup>*#</sup>, Haoyang Li, Hao Ge, Xuanyu Wang, Jiawen Niu, Yaofeng Tu, Bin Cui<sup>#</sup>
<br>
<b>VLDB 2025</b>
</li>

<li>
<i>
PS-MI: Accurate, Efficient, and Private Data Valuation in Vertical Federated Learning
</i>
<br>
Xiaokai Zhou, Xiao Yan, <b>Fangcheng Fu</b>, Ziwen Fu, Tieyun Qian, Yuanyuan Zhu, Qinbo Zhang, Bin Cui, Jiawei Jiang
<br>
<b>VLDB 2025</b>
</li>

<li>
<i>
FlexSP: Accelerating Large Language Model Training via Flexible Sequence Parallelism
</i>
<br>
Yujie Wang, Shiju Wang, Shenhan Zhu, <b>Fangcheng Fu</b><sup>#</sup>, Xinyi Liu, Xuefeng Xiao, Huixia Li, Jiashi Li, Faming Wu, Bin Cui<sup>#</sup>
<br>
<b>ASPLOS 2025</b>
</li>

<li>
<i>
Spindle: Efficient Distributed Training of Multi-Task Large Models via Wavefront Scheduling
</i>
<br>
Yujie Wang, Shenhan Zhu, <b>Fangcheng Fu</b><sup>#</sup>, Xupeng Miao<sup>#</sup>, Jie Zhang, Juan Zhu, Fan Hong, Yong Li, Bin Cui<sup>#</sup>
<br>
<b>ASPLOS 2025</b>
</li>

<li>
<i>
ByteScale: Communication-Efficient Scaling of LLM Training with a 2048K Context Length on 16384 GPUs
</i>
<br>
Hao Ge<sup>*</sup>, Junda Feng<sup>*</sup>, Qi Huang<sup>*</sup>, <b>Fangcheng Fu</b><sup>#</sup>, Xiaonan Nie, Lei Zuo, Haibin Lin<sup>#</sup>, Bin Cui<sup>#</sup>, Xin Liu<sup>#</sup>
<br>
<b>SIGCOMM 2025</b>
</li>

<li>
<i>
ThunderServe: High-performance and Cost-efficient LLM Serving in Cloud Environments
</i>
<br>
Youhe Jiang<sup>*</sup>, <b>Fangcheng Fu</b><sup>*</sup>, Xiaozhe Yao<sup>*</sup>, Taiyi Wang, Bin Cui, Ana Klimovic, Eiko Yoneki
<br>
<b>MLSys 2025</b>
</li>

<li>
<i>
Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs
</i>
<br>
Youhe Jiang<sup>*</sup>, <b>Fangcheng Fu</b><sup>*</sup>, Xiaozhe Yao<sup>*</sup>, Guoliang He<sup>*</sup>, Xupeng Miao, Ana Klimovic, Bin Cui, Binhang Yuan, Eiko Yoneki
<br>
<b>ICML 2025</b>
</li>

<li>
<i>
NetMoE: Accelerating MoE Training through Dynamic Sample Placement
</i>
<br>
Xinyi Liu, Yujie Wang, <b>Fangcheng Fu</b>, Xupeng Miao, Shenhan Zhu, Xiaonan Nie, Bin Cui
<br>
<b>ICLR 2025 (Spotlight)</b>
</li>

<li>
<i>
Training-free and Adaptive Sparse Attention for Efficient Long Video Generation
</i>
<br>
Yifei Xia, Suhan Ling, <b>Fangcheng Fu</b>, Yujie Wang, Huixia Li, Xuefeng Xiao, Bin Cui 
<br>
<b>ICCV 2025</b>
</li>

<li>
<i>
Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning
</i>
<br>
Peichao Lai, Zhengfeng Zhang, Wentao Zhang, <b>Fangcheng Fu</b>, Bin Cui
<br>
<b>ACL 2025</b>
</li>

<li>
<i>
Towards Scalable and Efficient Graph Structure Learning
</i>
<br>
Siqi Shen, Wentao Zhang, Chengshuo Du, Chong Chen, <b>Fangcheng Fu</b>, Yingxia, Shao, Bin Cui
<br>
<b>ICDE 2025</b>
</li>

<li>
<i>
Hounding Data Diversity: Towards Participant Selection in Vertical Federated Learning
</i>
<br>
Xiaokai Zhou, Xiao Yan, <b>Fangcheng Fu</b>, Xinyan Li, Hao Huang, Quanqing Xu, Chuanhui Yang, Bo Du, Tieyun Qian, Jiawei Jiang
<br>
<b>ICDE 2025</b>
</li>

<li>
<i>
Detecting and Analyzing Motifs in Large-scale Online Transaction Networks
</i>
<br>
Jiawei Jiang, Hao Huang, Zhigao Zheng, Yi Wei, <b>Fangcheng Fu</b>, Xiaosen Li, Bin Cui
<br>
<b>TKDE 37(2): 584-596 (2025)</b>
</li>

<li>
<i>
HaCore: Efficient Coreset Construction with Locality Sensitive Hashing for Vertical Federated Learning
</i>
<br>
Qinbo Zhang, Xiao Yan, Yukai Ding, <b>Fangcheng Fu</b>, Quanqing Xu, Ziyi Li, Chuang Hu, Jiawei Jiang 
<br>
<b>AAAI 2025</b>
</li>

<li>
<i>
Model Rake: A Defense Against Stealing Attacks in Split Learning
</i>
<br>
Qinbo Zhang, Xiao Yan, Yanfeng Zhao, <b>Fangcheng Fu</b>, Quanqing Xu, Yukai Ding, Xiaokai Zhou, Chuang Hu, Jiawei Jiang
<br>
<b>IJCAI 2025</b>
</li>

<li>
<i>
RAP: Random Projection is What You Need for Vertical Federated Learning
</i>
<br>
Qinbo Zhang, Xiao Yan, Yukai Ding, <b>Fangcheng Fu</b>, Chuang Hu, Quanqing Xu, Xu Chen, Jiawei Jiang
<br>
<b>DASFFA 2025</b>
</li>

<li>
<i>
Improving Low-Resource Sequence Labeling with Knowledge Fusion and Contextual Label Explanations
</i>
<br>
Peichao Lai, Jiaxin Gan, Feiyang Ye, Wentao Zhang, Fangcheng Fu, Yilei Wang, Bin Cui
<br>
<b>EMNLP 2025</b>
</li>

</ul>

<p><h2>2024</h2></p>

<ul>

<li>
<i>
Enabling Parallelism Hot Switching for Efficient Training of Large Language Models
</i>
<br>
Hao Ge<sup>*</sup>, <b>Fangcheng Fu</b><sup>*#</sup>, Haoyang Li, Xuanyu Wang, Sheng Lin, Yujie Wang, Xiaonan Nie, Hailin Zhang, Xupeng Miao, Bin Cui<sup>#</sup>
<br>
<b>SOSP 2024</b>
</li>

<li>
<i>
Efficient Multi-task LLM Quantization and Serving for Multiple LoRA Adapters
</i>
<br>
Yifei Xia, <b>Fangcheng Fu</b><sup>#</sup>, Wentao Zhang, Jiawei Jiang, Bin Cui<sup>#</sup>
<br>
<b>NeurIPS 2024</b>
</li>

<li>
<i>
LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing
</i>
<br>
Xiaonan Nie, Qibin Liu, <b>Fangcheng Fu</b><sup>#</sup>, Shenhan Zhu, Xupeng Miao, Xiaoyang Li, Yang Zhang, Shouda Liu, Bin Cui<sup>#</sup>
<br>
<b>NeurIPS 2024</b>
</li>

<li>
<i>
ProjPert: Projection-based Perturbation for Label Protection in Split Learning based Vertical Federated Learning
</i>
<br>
<b>Fangcheng Fu</b>, Xuanyu Wang, Jiawei Jiang, Huanran Xue, and Bin Cui
<br>
<b>TKDE 36(7): 3417-3428 (2024)</b>
</li>

<li>
<i>
Improving Automatic Parallel Training via Balanced Memory Workload Optimization
</i>
<br>
Yujie Wang, Youhe Jiang, Xupeng Miao<sup>#</sup>, <b>Fangcheng Fu</b><sup>#</sup>, Shenhan Zhu, Xiaonan Nie, Yaofeng Tu, Bin Cui<sup>#</sup>
<br>
<b>TKDE 36(8): 3906-3920 (2024)</b>
</li>

<li>
<i>
Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion Inference
</i>
<br>
Zihao Yu, Haoyang Li, <b>Fangcheng Fu</b>, Xupeng Miao, Bin Cui
<br>
<b>AAAI 2024</b>
</li>

<li>
<i>
X-former Elucidator: Reviving Efficient Attention for Long Context Language Modeling
</i>
<br>
Xupeng Miao, Shenhan Zhu, <b>Fangcheng Fu</b>, Ziyu Guo, Zhi Yang, Yaofeng Tu, Zhihao Jia, Bin Cui
<br>
<b>IJCAI 2024</b>
</li>

<li>
<i>
Generative and Contrastive Paradigms Are Complementary for Graph Self-Supervised Learning
</i>
<br>
Yuxiang Wang, Xiao Yan, Chuang Hu, Quanqing Xu, Chuanhui Yang, <b>Fangcheng Fu</b>, Wentao Zhang, Hao Wang, Bo Du, Jiawei Jiang
<br>
<b>ICDE 2024</b>
</li>

</ul>


<p><h2>2023</h2></p>

<ul>

<li>
<i>
Angel-PTM: A Scalable and Economical Large-scale Pre-training System in Tencent
</i>
<br>
Xiaonan Nie, Yi Liu, <b>Fangcheng Fu</b><sup>#</sup>, Jinbao Xue, Dian Jiao, Xupeng Miao, Yangyu Tao, Bin Cui<sup>#</sup>
<br>
<b>VLDB 2023</b>
</li>

<li>
<i>
OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning
</i>
<br>
Youhe Jiang, <b>Fangcheng Fu</b><sup>#</sup>, Xupeng Miao, Xiaonan Nie, Bin Cui<sup>#</sup>
<br>
<b>IJCAI 2023</b>
</li>

<li>
<i>
KVSAgg: Secure Aggregation of Distributed Key-Value Sets
</i>
<br>
Yuhan Wu, Siyuan Dong, Yi Zhou, Yikai Zhao, <b>Fangcheng Fu</b>, Tong Yang, Chaoyue Niu, Fan Wu, Bin Cui
<br>
<b>ICDE 2023</b>
</li>

<li>
<i>
P2CG: A Privacy Preserving Collaborative Graph Neural Network Training Framework
</i>
<br>
Xupeng Miao, Wentao Zhang, Yuezihan Jiang, <b>Fangcheng Fu</b>, Yingxia Shao, Lei Chen, Yangyu Tao, Gang Cao, Bin Cui
<br>
<b>VLDB Journal 32(4): 717-736 (2023)</b>
</li>

<li>
<i>
Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion Inference
</i>
<br>
Zihao Yu, Haoyang Li, <b>Fangcheng Fu</b>, Xupeng Miao, Bin Cui
<br>
<b>MLSys Workshop NeurIPS 2023</b>
</li>

</ul>

<p><h2>2022</h2></p>

<ul>

<li>
<i>
Towards Communication-efficient Vertical Federated Learning Training via Cache-enabled Local Update
</i>
<br>
<b>Fangcheng Fu</b>, Xupeng Miao, Jiawei Jiang, Huanran Xue, Bin Cui
<br>
<b>VLDB 2022</b>
</li>

<li>
<i>
BlindFL: Vertical Federated Machine Learning without Peeking into Your Data
</i>
<br>
<b>Fangcheng Fu</b>, Huanran Xue, Yong Cheng, Yangyu Tao, Bin Cui
<br>
<b>SIGMOD 2022</b>
</li>

<li>
<i>
VF-PS: How to Select Important Participants in Vertical Federated Learning, Efficiently and Securely?
</i>
<br>
Jiawei Jiang, Lukas Burkhalter, <b>Fangcheng Fu</b>, Bo Li, Bolin Ding, Bo Du, Anwar Hithnawi, Ce Zhang
<br>
<b>NeurIPS 2022</b>
</li>

<li>
<i>
Analyzing Online Transaction Networks with Network Motifs
</i>
<br>
Jiawei Jiang, Yusong Hu, Xiaosen Li, Wen Ouyang, Zhitao Wang, <b>Fangcheng Fu</b>, Bin Cui
<br>
<b>SIGKDD 2022</b>
</li>

<li>
<i>
K-Core Decomposition on Super Large Graphs with Limited Resources
</i>
<br>
Shicheng Gao, Jie Xu, Xiaosen Li, <b>Fangcheng Fu</b>, Wentao Zhang, Wen Ouyang, Yangyu Tao, Bin Cui
<br>
<b>ACM SAC 2022</b>
</li>

</ul>

<p><h2>2021</h2></p>

<ul>

<li>
<i>
VF<sup>2</sup>Boost: Very Fast Vertical Federated Gradient Boosting for Cross-Enterprise Learning
</i>
<br>
<b>Fangcheng Fu</b>, Yingxia Shao, Lele Yu, Jiawei Jiang, Huanran Xue, Yangyu Tao, Bin Cui
<br>
<b>SIGMOD 2021</b>
</li>

</ul>

<p><h2>2020</h2></p>

<ul>

<li>
<i>
Don’t Waste Your Bits! Squeeze Activations and Gradients for Deep Neural Networks via TinyScript
</i>
<br>
<b>Fangcheng Fu</b>, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce Zhang, Bin Cui
<br>
<b>ICML 2020</b>
</li>

<li>
<i>
SKCompress: Compressing Sparse and Nonuniform Gradient in Distributed Machine Learning
</i>
<br>
Jiawei Jiang<sup>*</sup>, <b>Fangcheng Fu</b><sup>*</sup>, Tong Yang, Yingxia Shao, Bin Cui
<br>
<b>VLDB Journal 29(5): 945-972 (2020)</b>
</li>

</ul>

<p><h2>2019</h2></p>

<ul>

<li>
<i>
An Experimental Evaluation of Large Scale GBDT Systems
</i>
<br>
<b>Fangcheng Fu</b>, Jiawei Jiang, Yingxia Shao, Bin Cui
<br>
<b>VLDB 2019</b>
</li>

</ul>

<p><h2>2018</h2></p>

<ul>

<li>
<i>
SketchML: Accelerating Distributed Machine Learning with Data Sketches
</i>
<br>
Jiawei Jiang, <b>Fangcheng Fu</b>, Tong Yang, Bin Cui
<br>
<b>SIGMOD 2018</b>
</li>

<li>
<i>
DimBoost: Boosting Gradient Boosting Tree to Higher Dimensions
</i>
<br>
Jiawei Jiang, Bin Cui, Ce Zhang, <b>Fangcheng Fu</b>
<br>
<b>SIGMOD 2018</b>
</li>

</ul>

<p><h2>Papers in Chinese</h2></p>

<ul>

<li>
<i>
MQLserve：基于量化的多任务大语言模型服务系统/MQLserve: Quantization-based Multi-task LLM serve system
</i>
<br>
<b>符芳诚</b>,夏义扉,崔斌/<b>Fangcheng Fu</b>, Yifei Xia, Bin Cui
<br>
<b>计算机学报/Chinese Journal of Computers, 2025, 48(3):517-536 <i>(NDBC 2024 Best Paper)</i></b>
</li>

<li>
<i>
面向高维特征和多分类的分布式梯度提升树/Distributed Gradient Boosting Decision Tree Algorithm for High-dimensional and Multi-classification Problems
</i>
<br>
江佳伟,<b>符芳诚</b>,邵蓥侠,崔斌/Jiawei Jiang, <b>Fangcheng Fu</b>, Yingxia Shao, Bin Cui
<br>
<b>软件学报/Journal of Software, 2019, 30(3):784-798</b>
</li>

</ul>

<p><h1>Patents</h1></p>

<ul>
<li>
基于深度神经网络最小方差梯度量化压缩及图像处理方法. ZL 2019 1 1029711.0
</il>
<li>
一种数据处理方法、装置、设备及计算机可读存储介质. ZL 2021 1 0576191.6
</il>
<li>
基于联邦学习的数据传输方法、装置以及可读存储介质. ZL 2021 1 0680161.X
</il>
<li>
基于联邦神经网络模型的数据处理方法、相关设备及介质. ZL 2021 1 0531392.4
</il>
<li>
联邦模型训练方法、装置、终端设备以及存储介质. ZL 2022 1 0363190.8
</il>
<li>
多方安全计算方法、装置、设备及存储介质. ZL 2021 1 0503941.7
</il>
<li>
联邦神经网络模型的训练方法、装置、设备及存储介质. ZL 2020 1 1167325.0
</il>
<li>
数据集合处理方法、数据处理方法、装置及存储介质. ZL 2021 1 0541183.8
</il>
</ul>

{{< /rawhtml >}}
